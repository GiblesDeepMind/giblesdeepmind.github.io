---

title:  "[통계학] 다중공선성에 대한 고찰 - 해석과 모델 성능의 관점에서"
excerpt: "다중공선성"

toc: true
toc_sticky: true
toc_label: "페이지 주요 목차"

#classes: wide

# layout: posts

date:   2020-08-24 01:21:25 +0900
categories: 
  - Statistics
tags: 
  - Multicollinearity
  - 다중공선성
  - 다중공선성 회귀분석
  - 회귀분석 다중공선성
  - 머신러닝 다중공선성
  - 다중공선성 머신러닝
#last_modified_at: 2020-07-19T00:40:00-05:00

# 포스팅 마크다운 상단 최상단에 표시
star: true 
use_math: true

---



# 1. 어디로 가야하오?
![](https://giblesdeepmind.github.io/assets/images/Statistics/Multicollinearity/leesin.png){: .align-center}

- 다중공선성을 만난 우리들

본 글에서는 "어디로 가야하오" 에 담긴 두 가지 의미를 다룰 것이다.

1. 다중공선성을 꼭 파악해야하는가?
2. 반드시 해결해야하는 문제인가?

```
본 글에서는 다중공선성에 대한 해결 방법을 다루지는 않는다.

왜냐하면, VIF / 상관 / PCA / 도메인 / 종속변수와 연관성 등등 다양한 방법이 존재하고 실제로 해당 방법을 적용하는 과정 자체는 어렵지 않기 때문이다.

따라서 본 글의 목적은 다음과 같다.
1. 다중공선성이 모델에 실질적으로 어떤 영향을 미칠수 있는지에 대해 이해한다.
2. 모델 해석 / 모델 성능 두 가지 관점에서 다중공선성을 판단해본다. 
```

---
# 2. 회귀분석 A씨와 다중공선성 B씨의 소개팅

<br>회귀분석 A씨는 들뜬 마음으로 다중공선성 B씨와의 소개팅을 준비중이었다. 그러다 문득 이런 생각이 들었다. </br>

"남들도 다 한번쯤은 만난다고해서 만나기는 하는데 괜히 시간만 버리면 어쩌지?"

A씨 생각대로 정말 시간 낭비일까?

## 2-1. 다중공선성B씨의 사정

<br>B씨는 메갓 선수의 퍼포먼스를 매 경기마다 측정하는 퍼포먼스 분석가이다. </br>

어느날 B씨는 수집한 데이터를 기반으로 지난 1년간 메갓 선수의 숏 패스 / 롱 패스 / 쓰루 패스 성공률 추이를 살펴봤다.

분석 결과 각 변수들 간의 상관이 매우 높은 것으로 나타났다. 즉, 숏 패스의 성공률이 저조하면 롱 패스와 쓰루 패스의 성공률도 저조하고 롱 패스의 성공률이 높으면 숏 패스와 쓰루 패스의 성공률도 높다는 사실을 발견했다.

B씨에게는 매우 의미있는 행위이다. 왜냐하면, 세 가지 패스가 상관이 있다고 해서 한 가지 패스만을 측정할 수는 없기 때문이다. 숏 패스, 롱 패스, 쓰루 패스는 각각 고유한 의미를 가진다.

하지만, 다른 사람들의 생각은 다르다. "비슷한 정보를 가지고 있으면 하나만 사용하는게 낫지 않은가?"

## 2-2. 회귀 계수의 신뢰 구간

<br>다중공선성은 B씨의 분석 결과와 같이 여러 변수들이 비슷한 정보를 공유하는 현상을 말한다. </br>

![](https://giblesdeepmind.github.io/assets/images/Statistics/Multicollinearity/diagram.JPG){: .align-center}

이제 다중공선성이 무엇인지 대략적인 감을 잡았을 것이다. 

본격적으로 들어가기전에 다음 질문에 대해서 한번 생각해보자.

"회귀 모델을 어떻게 신뢰할 수 있는가?" 

다들 각자만의 생각이 있겠지만,
- $R^2$가 높은게 짱이야!
- 아니야 AIC랑 BIC가 낮아야 해
- etc

이 문제에 대해서 원초적으로 생각해보자. 

회귀 분석에는 아웃풋을 도출하는 명확한 함수식이 존재한다. 그리고, 우리는 이 함수식을 통해 어떤 결론을 내리게 된다. 

아래와 같은 가상의 모델을 가정해보자.

$Y= 0.7X_1 - 0.3X_2 + 0.2$

- Y : 실점 횟수
- $X_1$ : 스트레스
- $X_2$ : 수비훈련기간

모델에서 스트레스 점수가 높을수록 실점 횟수가 증가하지만 수비훈련기간에 따라 그 영향력이 경감될 수 있음을 알 수 있다.

하지만, 여기서 끝이 아니다. **회귀 계수가 안정적이어야만 위와 같은 결론을 내릴 수 있다.** 따라서 회귀 계수 신뢰 구간을 확인할 필요가 있다.

만약, X1 변수의 회귀 계수 $\beta_2$의 신뢰 구간이 다음과 같다고 가정해보자.

$-0.3 - 0.4 <\beta_2 < -0.3 + 0.4$

여기서 중요한 포인트는 두 가지 이다.

* 회귀 계수의 신뢰 구간에 0이 포함된다.
  * $\hat{\beta} 값이 0일 수 있다.$
  * 즉, 변수가 유의미하지 않다.
* 회귀 계수가 +가 될 수도 있다.
  * 해석의 방향이 반대가 될 수 있다.

이처럼 신뢰 구간이 넓어지면 계수 해석 방향이 달라 질 수 있다.

## 2-3. 다중공선성과 회귀 계수의 신뢰 구간

<br>**<span style="color:red">다중공선성은 회귀 계수의 신뢰 구간을 넓힐 수 있다.</span>** </br>

그렇기 때문에 우리는 시간을 투자하여 다중공선성을 살펴봐야 한다.

$\hat{\beta_2}$의 분산식은 다음과 같다.

$Var[\hat{\beta_2}]=\sigma^2({ X }^{ T }X)^{-1}$ 

$\sigma^2$는 오차항의 분산이라 MSE로 추정한다. 

$({ X }^{ T }X)^{-1}$에 대해 생각하기전에 $X$가 다음과 같은 Matrix라고 가정해보자. 

$$X=\begin{pmatrix} 1&1&0 \\ 0&1&1 \\ 0&0&0 \end{pmatrix}$$

두 번재 열은 첫 번째 열과 세 번째 열의 선형 결합으로 표현된다. 이렇게 될 경우 ${ X }^{ T }X$의 역행렬이 존재하지 않게 된다. 

역행렬이 존재 하지 않는 다는 것은 바꿔 얘기하면 Matrix안에 Free variable이 있다는 뜻이고 그로 인해 유일해가 존재하지 않는다는 의미가 된다. 

자세한 내용은 아래 페이지를 참고하면 좋을 것 같다.
> [Full Column Rank](https://twlab.tistory.com/22)

위 경우는 매우 극단적인 경우이다. 하지만, 위와 같은 경우가 아니더라도 한 변수가 나머지 변수들과 매우 흡사한 경우가 있을 수 있다.

이때, $({ X }^{ T }X)^{-1}$의 역행렬은 다음과 같이 계산된다.

$({ X }^{ T }X)^{-1} = \frac{1}{det(A)}adj(A)$

**여기서 $det(A)$가 0에 가까워질수록(=선형종속적인 성격을 띌수록) $({ X }^{ T }X)^{-1}$의 값이 커지게 되고 $Var[\hat{\beta_2}]$이 커지게 된다.**

> $det(A)$가 0에 가까워질수록?
>> 역행렬 존재 조건이 $det(A) = 0$이라는 것을 생각해보자. 앞서 얘기했듯이 한 열이 다른 열들의 선형 결합으로 표현될 경우 역행렬이 존재하지 않게 된다. 이 말을 바꿔서 생각하면 다른 열들의 선형 결합이 한 열을 근사하게 표현할 경우 $det(A)$값이 0에 가까워진다는 의미가 된다. 

---
# 3. 다중공선성은 정말로 위협적인 존재인가?


<br>앞서 다중공선성으로 인해 회귀 계수의 신뢰 구간이 넓어질 수 있다는 얘기를 했다.</br>

이번 챕터에서는 우리가 다중공선성으로 인해 실질적으로 영향을 받을 수 있는지에 대해 두 가지 관점에서 알아볼 것이다.

1. 예측(모델 성능)
2. 해석(추론)

## 3-1. 다중공선성은 예측 성능에 어떤 영향을 미치는가? 

<br>결론부터 얘기하면 예측 성능 자체에는 큰 영향을 미치지 않을 수 있다. </br>

여기서 예측 성능이란 예측값을 뜻한다. 그림을 통해 살펴보자.

![](https://giblesdeepmind.github.io/assets/images/Statistics/Multicollinearity/Multi_space.JPG){: .align-center}
> Applied Linear Statistical Models
>> 다중공선성이 존재할 때 해 공간 예시

상단의 식 $X_2 = 5 + .5X_1$은 $X_2$과 $X_1$의 상관이 높다는 것을 의미한다.

그림에서 해집합을 표현할 수 있는 평면은 2개(그 이상일 수 있음)이지만 해집합 자체(빨간선)는 하나이다. 

다시 말하자면 공선성이 발생하여 회귀 계수가 넓은 신뢰 구간을 가질 때, 하나의 값을 추정하는 여러 회귀식이 존재할 수 있다.

### 정말 예측 성능에는 영향을 미치지 않을까?

<br>다중공선성이 예측 성능에 큰 영향을 미치지 않는다고 말하려면 한 가지 전제가 더 필요하다.</br>

ML 모델에서 데이터는 크게 훈련/테스트 셋으로 나누어진다.

만약, 데이터 수가 너무 적어 훈련 셋과 테스트 셋 간에 공분산 구조가 다르다면 다중공선성은 또 다른 문제를 야기할 수 있을 것이다.

> 공분산 구조가 다르다는 것은 다음의 의미이다. 예를 들어, 샘플 수가 13개 밖에 안되고 이 중 4명은 남자 9명은 여자인 데이터 셋을 가정해보자. 이때, 훈련 셋과 테스트 셋으로 데이터를 분할할 경우 테스트 셋에 모든 남자가 포함될 수도 있다.
>> 이럴 경우 모델이 여자에 대한 정보만 학습하고 추론은 남자에 대한 정보를 바탕으로 하게되는 상황이 발생할 수 있다.
>>>즉, 훈련 셋에서 반영된 변수 간 다중공선성 정도와 테스트 셋에서 반영될 다중공선성 정도가 다를 수 있다.
>>>> 그리고 이와 같은 차이는 훈련 셋과 테스트 셋 간 공분산 구조가 다를 경우 발생할 수 있다.

결론적으로 훈련 셋과 테스트 셋 간 공분산 구조에 큰 차이가 없을 경우
>(훈련 셋에서 발생하는 다중공선성과 테스트 셋에서 발생하는 다중 공선성에 큰 차이가 없는 경우) 

다중공선성은 모델의 예측 능력에 큰 영향을 미치지 않을 수 있다.

### 사례. 다중공선성을 지닌 데이터를 의사결정나무에 적용하는 경우

<br>Piramuthu(2008)는 데이터에 아웃라이어가 존재하는 경우 / 이상치가 존재하는 경우 / 다중공선성이 있는 경우 등으로 조건을 나누어 의사결정나무에 어떤 영향을 미치는지를 확인하는 시뮬레이션 연구 결과를 진행했다.</br>

시뮬레이션 결과 다중공선성이 있는 변수를 제거했을 때 오히려 모델 성능이 하락한 것을 확인할 수 있다.

![](https://giblesdeepmind.github.io/assets/images/Statistics/Multicollinearity/prediction_error_tree.JPG){: .align-center}

> 다른 변수들과 상관이 있는 $x_4$를 제거했을 때 예측 오류가 더 높아짐

물론, 위 사례를 모든 경우에 일반화 할 수는 없다. 하지만, 다음의 사실을 시사하기에는 충분한 사례라고 생각한다.

* 모델 성능 향상이 목적인 경우 두었을 때 다중공선성 만으로 변수를 제거하면 오히려 성능이 떨어질 수도 있다. 

## 3-2. 다중공선성은 모델 해석에 어떤 영향을 미치는가?

<br>2-2에서 살펴봤듯이 다중공선성이 발생하면 회귀 계수의 신뢰 구간이 넓어진다. 같은 사례로 이 내용을 한번 더 복기해보자. </br>

$Y= 0.7X_1  -0.3X_2 + 0.2$

- Y : 실점 횟수
- $X_1$ : 스트레스
- $X_2$ : 수비훈련기간

$-0.3 - 0.4 <\beta_2 < -0.3 + 0.4$

당신은 토트넘의 감독 무리뉴다. 팀 분석가한테 `요즘 골이 많이 먹히는 것 같아`라고 고민을 말했더니 분석가가 위와 같은 모델을 들고 왔다.

모델을 보고 만족한 무리뉴는 분석가에게 `약팀이랑 붙었던 경기는 빼고 다시 결과를 말해줘`라고 부탁했다. 

그랬더니, $Y= 0.8X_1 + 0.2X_2 + 0.3$ 와 같은 모델이 나왔다. 

이때, 당신은 분석가에게 뭐라고 할 것인가?

1. $\beta_2$의 계수가 들쑥날쑥 하는군 역시 축구의 세계는 알다가도 모르는 것이야
2. 훈련이 별로 효과가 없나봐
3. 그동안 정말 고생이 많았네. 2만원을 줄테니 오늘 저녁은 치킨을 먹고 내일부터는 안나와도 된다네

선택은 이 글을 읽는 분들께 맡기겠다.

다시 한번 얘기하지만, 다중공선성이 발생하면 회귀 계수의 신뢰 구간이 넓어진다. 

이는 두 가지 문제를 초래한다.

1. 신뢰 구간이 넓을 수록 회귀 계수를 신뢰할 수 없다.(=회귀 계수를 이용해 판단을 하기 어렵다)
2. 모델의 성능이 좋아도 성능의 원인을 명확히 파악하기 어렵다.(비슷한 특징을 가진 여러 변수들이 모델에 존재하기 때문에)

### 다중공선성을 제거하면 정말 모델에 대한 해석력이 좋아지는가?

<br>다중공선성을 제거한다고 해서 꼭 모델에 대한 해석력이 높아지는 것은 아니다.</br>

대표적인 사례로 상호작용항이 있다.

상호작용항은 다음의 의미를 가진다. 
> 한 변수의 종속변수에 대한 영향력이 다른 변수의 수준에 따라 달라지는 것
>> 두 변수의 곱으로 표현 $\beta_3XZ$

상호작용항은 보통 곱셈상호작용항으로 표현하기 때문에 본질적으로 다른 변수들과 공선성을 가질 수 밖에 없다.

이때, 공선성만으로 변수를 제거하게 되면 회귀선이 데이터를 제대로 적합하지 못할 수 있다(Brambor, Clark, Golder, 2006).

> 자세한 내용은 Refrence 논문 참조

또한, 가설의 형태에도 문제가 생긴다.

$Y =\hat{\beta_0} +\hat{\beta_1}X + \hat{\beta_2}Z + \hat{\beta_3}XZ + \varepsilon$

위와 같은 회귀식의 경우 두 개의 주효과와 한 개의 상호작용효과에 대한 가설이 나올 수 있다.

하지만, 공선성만을 이유로 임의로 변수를 제거할 경우 가설의 형태에 변화가 생길 수 있다.

---
# 4.결론

<br>1장에서 처음 했던 질문을 다시 살펴보자.</br>


## 4-1. 다중공선성을 꼭 파악해야하는가?
<br>다중공선성은 시간을 투자할만한 가치가 있는 녀석이다.</br>

왜냐하면, 다중공선성을 확인하는 과정에서 여러 인사이트를 얻을 수 있기 때문이다.

1. 차원 축소
2. 비슷한 성질을 지니는 변수 그룹화
3. 등등

## 4-2. 반드시 해결해야하는 문제인가?

<br>개인적인 대답은 No이다.</br>

상황에 따라 중요다가 다르기 때문이다. 

다중공선성이 모델에 영향을 미치는 부분은 신뢰구간의 넓이이다. 해석이 중요할수록 해당 이슈는 크리티컬 할 수 있다. 하지만, 예측력이 중요한 관심사일 경우 크게 중요하지 않을 수 있다.

---
# Reference
* [Collinearity](https://genomicsclass.github.io/book/pages/collinearity.html)
* [Multicollinearity: Why does highly correlated columns in the design matrix lead to high variance of the regression coefficient?](https://math.stackexchange.com/questions/1178431/multicollinearity-why-does-highly-correlated-columns-in-the-design-matrix-lead)
* [Why do we use the term multicollinearity, when the vectors representing two variables are never truly collinear?](https://stats.stackexchange.com/questions/128735/why-do-we-use-the-term-multicollinearity-when-the-vectors-representing-two-vari)
* [Lecture 17: Multicollinearity](http://www.stat.cmu.edu/~larry/=stat401/lecture-17.pdf)
* [Is multicollinearity really a problem?](https://stats.stackexchange.com/questions/268966/is-multicollinearity-really-a-problem)
* [Linear Models: How does multicollinearity affect prediction accuracy?](https://www.quora.com/Linear-Models-How-does-multicollinearity-affect-prediction-accuracy)
* [Multicollinearity and predictive performance](https://stats.stackexchange.com/questions/361247/multicollinearity-and-predictive-performance)
* Kutner, M. H., Nachtsheim, C. J., Neter, J., & Li, W. (2005). Applied linear statistical models (Vol. 5). New York: McGraw-Hill Irwin.
* Piramuthu, S. (2008). Input data for decision trees. Expert Systems with applications, 34(2), 1220-1226.
* Brambor, T., Clark, W. R., & Golder, M. (2006). Understanding interaction models: Improving empirical analyses. Political analysis, 63-82.